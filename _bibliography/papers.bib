---
---


@inproceedings{ICML/MaZouLiu25,
  abbr={ICML},
  author={Xinsong Ma and Xin Zou and Weiwei Liu},
  title={A Online Statistical Framework for Out-of-Distribution Detection},
  booktitle={ICML},
  year={2025},
  abstract={Out-of-distribution (OOD) detection task is significant in reliable and safety-critical applications. Existing approaches primarily focus on developing the powerful score function, but overlook the design of decision-making rules based on these score function. In contrast to prior studies, we rethink the OOD detection task from an perspective of online multiple hypothesis testing. We then propose a novel generalized LOND (g-LOND) algorithm to solve the above problem. Theoretically, the g-LOND algorithm controls false discovery rate (FDR) at pre-specified level without the consideration for the dependence between the p-values. Furthermore, we prove that the false positive rate (FPR) of the g-LOND algorithm converges to zero in probability based on the generalized Gaussian-like distribution family. Finally, the extensive experimental results verify the effectiveness of g-LOND algorithm for OOD detection.},
  selected={false},
  html={https://openreview.net/forum?id=BnPaSXSmz1},
  pdf={https://openreview.net/pdf?id=BnPaSXSmz1},
}

@inproceedings{DBLP:conf/nips/ZouX2024,
  abbr={NeurIPS},
  author       = {Xin Zou and
                  Zhengyu Zhou and
                  Jingyuan Xu and
                  Weiwei Liu},
  title        = {A Boosting-Type Convergence Result for AdaBoost.MH with Factorized Multi-Class Classifiers},
  booktitle    = {NeurIPS},
  year         = {2024},
  abstract={AdaBoost is a well-known algorithm in boosting. Schapire and Singer propose, an extension of AdaBoost, named AdaBoost.MH, for multi-class classification problems. Kégl shows empirically that AdaBoost.MH works better when the classical one-against-all base classifiers are replaced by factorized base classifiers containing a binary classifier and a vote (or code) vector. However, the factorization makes it much more difficult to provide a convergence result for the factorized version of AdaBoost.MH. Then, Kégl raises an open problem in COLT 2014 to look for a convergence result for the factorized AdaBoost.MH. In this work, we resolve this open problem by presenting a convergence result for AdaBoost.MH with factorized multi-class classifiers.},
  selected={true},
  eqauthors={Xin Zou, Zhengyu Zhou},
  html={https://openreview.net/forum?id=7Lv8zHQWwS},
  pdf={https://openreview.net/pdf?id=7Lv8zHQWwS},
}

@article{LI2024111008,
  abbr={PR},
  title = {Residual network with self-adaptive time step size},
  journal = {Pattern Recognition},
  pages = {111008},
  year = {2024},
  issn = {0031-3203},
  doi = {https://doi.org/10.1016/j.patcog.2024.111008},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320324007593},
  author = {Xiyuan Li and Xin Zou and Weiwei Liu},
  abstract = {Residual Networks (ResNet) are pivotal in machine learning. The connection between ResNets and ordinary differential equations (ODEs) has inspired enhancements of ResNets using sophisticated numerical methods for ODE systems. Recent advancements in numerical self-adaptive schemes, which adjust time step sizes based on the feature maps or parameters of residual blocks, have demonstrated promising results in enhancing ResNet performance, surpassing those achieved with fixed time step methods. However, these self-adaptive time step constructions lack theoretical support and can limit performance improvements since the self-adaptive time steps should theoretically depend on both the feature maps and the parameters of the residual blocks. In this study, we conduct a rigorous theoretical analysis of the residual functions associated with fixed or self-adaptive time step methods to demonstrate the advantages and rational designs of the self-adaptive approach. Subsequently, we introduce a novel self-adaptive ResNet, AdaTS-ResNet, which effectively incorporates both feature maps and parameters in its time step adjustments. Experimental results on the ImageNet and CIFAR datasets reveal that AdaTS-ResNet surpasses TSCLSTM-ResNet (Yang et al., 2020) in prediction accuracy and computational efficiency, where TSCLSTM represents the latest advancements of the methods designed based on the self-adaptive scheme of ODE. Our findings highlight the potential of improving ResNet architectures through adaptive techniques of dynamical systems, offering insights for future enhancements in deep learning models.},
  selected={false},
  html={https://www.sciencedirect.com/science/article/pii/S0031320324007593},
  pdf={https://www.sciencedirect.com/science/article/pii/S0031320324007593/pdfft?md5=8051d1b2504a167e2f5a02ab9ac1bf2c&pid=1-s2.0-S0031320324007593-main.pdf},
}

@inproceedings{anonymous2024a,
  abbr={ICML},
  title={A Provable Decision Rule for Out-of-Distribution Detection},
  author={Xinsong Ma and Xin Zou and Weiwei Liu},
  booktitle={ICML},
  year={2024},
  abstract={
    Out-of-distribution (OOD) detection task plays the key role in reliable and safety-critical applications. Existing researches mainly devote to designing or training the powerful score function but overlook investigating the decision rule based on the proposed score function. Different from previous work, this paper aims to design a decision rule with rigorous theoretical guarantee and well empirical performance. Specifically, we provide a new insight for the OOD detection task from a hypothesis testing perspective and propose a novel generalized Benjamini Hochberg (g-BH) procedure to solve the testing problem. Theoretically, the g-BH procedure controls false discovery rate (FDR) at pre-specified level. Furthermore, we derive an upper bound of the expectation of false positive rate (FPR) for the g-BH procedure based on the tailed generalized Gaussian distribution family, indicating that the FPR of g-BH procedure converges to zero in probability. Finally, the extensive experimental results verify the superiority of g-BH procedure over the traditional threshold-based decision rule on several OOD detection benchmarks. Particularly, combining SHE with the g-BH procedure, the FPR95 is reduced by 13.65% on average compared with the vanilla SHE.},
    selected={false},
  html={https://openreview.net/forum?id=SPygKwms0X&referrer=%5Bthe%20profile%20of%20Weiwei%20Liu%5D(%2Fprofile%3Fid%3D~Weiwei_Liu1)},
  pdf={https://openreview.net/pdf?id=SPygKwms0X},
}

@inproceedings{ZouLiu2024,
  abbr    = {AAAI},
  title   = {Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data},
  author={Xin Zou and Weiwei Liu},
  booktitle = {AAAI},
  year={2024},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/29673},
  doi={10.1609/aaai.v38i15.29673},
  abstract  = {Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. In this paper, we study the confidence set prediction problem in the OOD generalization setting. Split conformal prediction (SCP) is an efficient framework for handling the confidence set prediction problem. However, the validity of SCP requires the examples to be exchangeable, which is violated in the OOD setting. Empirically, we show that trivially applying SCP results in a failure to maintain the marginal coverage when the unseen target domain is different from the source domain. To address this issue, we develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method. Finally, we conduct experiments on simulated data to empirically verify the correctness of our theory and the validity of our proposed method.},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/29673},
  number={15},
  volume={38}, 
  pages={17263-17270},
  selected={true},
  dimensions={true},
  pdf={https://ojs.aaai.org/index.php/AAAI/article/view/29673/31151},
}

@inproceedings{DBLP:conf/nips/ZouL23,
  abbr={NeurIPS},
  author = {Xin Zou and Weiwei Liu},
  title = {On the Adversarial Robustness of Out-of-distribution Generalization Models},
  booktitle = {NeurIPS},
  year = {2023},
  selected={true},
  abstract={Out-of-distribution (OOD) generalization has attracted increasing research attention
    in recent years, due to its promising experimental results in real-world applications.
    Interestingly, we find that existing OOD generalization methods are vulnerable
    to adversarial attacks. This motivates us to study OOD adversarial robustness.
    We first present theoretical analyses of OOD adversarial robustness in two different
    complementary settings. Motivated by the theoretical results, we design
    two algorithms to improve the OOD adversarial robustness. Finally, we conduct
    experiments to validate the effectiveness of our proposed algorithms. Our code is
    available at https://github.com/ZouXinn/OOD-Adv.},
  url = {https://neurips.cc/virtual/2023/poster/72104},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/d9888cc7baa04c2e44e8115588133515-Paper-Conference.pdf},
  html= {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d9888cc7baa04c2e44e8115588133515-Abstract-Conference.html},
}

@article{JMLR:v24:22-0866,
  abbr={JMLR},
  author  = {Xin Zou and Weiwei Liu},
  title   = {Generalization Bounds for Adversarial Contrastive Learning},
  journal = {JMLR},
  year    = {2023},
  volume  = {24},
  number  = {114},
  pages   = {1--54},
  dimensions={true},
  url     = {http://jmlr.org/papers/v24/22-0866.html},
  abstract={Deep networks are well-known to be fragile to adversarial attacks, and adversarial training is one of the most popular methods used to train a robust model. To take advantage of unlabeled data, recent works have applied adversarial training to contrastive learning (Adversarial Contrastive Learning; ACL for short) and obtain promising robust performance. However, the theory of ACL is not well understood. To fill this gap, we leverage the Rademacher omplexity to analyze the generalization performance of ACL, with a particular focus on linear models and multi-layer neural networks under $\ell_p$ attack ($p≥1$). Our theory shows that the average adversarial risk of the downstream tasks can be upper bounded by the adversarial unsupervised risk of the upstream task. The experimental results validate our theory.},
  selected={true},
  pdf={https://www.jmlr.org/papers/volume24/22-0866/22-0866.pdf},
  html={https://www.jmlr.org/papers/v24/22-0866.html},
}

@inproceedings{DBLP:conf/nips/LiXL22,
  abbr={NeurIPS},
  author       = {Xiyuan Li and
                  Xin Zou and
                  Weiwei Liu},
  title        = {Defending Against Adversarial Attacks via Neural Dynamic System},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  abstract={Although deep neural networks (DNN) have achieved great success, their applications in safety-critical areas are hindered due to their vulnerability to adversarial attacks. Some recent works have accordingly proposed to enhance the robustness of DNN from a dynamic system perspective. Following this line of inquiry, and inspired by the asymptotic stability of the general nonautonomous dynamical system, we propose to make each clean instance be the asymptotically stable equilibrium points of a slowly time-varying system in order to defend against adversarial attacks. We present a theoretical guarantee that if a clean instance is an asymptotically stable equilibrium point and the adversarial instance is in the neighborhood of this point, the asymptotic stability will reduce the adversarial noise to bring the adversarial instance close to the clean instance. Motivated by our theoretical results, we go on to propose a nonautonomous neural ordinary differential equation (ASODE) and place constraints on its corresponding linear time-variant system to make all clean instances act as its asymptotically stable equilibrium points. Our analysis suggests that the constraints can be converted to regularizers in implementation. The experimental results show that ASODE improves robustness against adversarial attacks and outperforms state-of-the-art methods.},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2022/file/299a08ee712d4752c890938da99a77c6-Paper-Conference.pdf},
  html={https://proceedings.neurips.cc/paper_files/paper/2022/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html},
  selected={false},
}